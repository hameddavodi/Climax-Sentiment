## Intro
The movie "Climax 2018" directed by Gasper Noe has been the subject of much critical acclaim and debate. To delve deeper into the movie's impact and meaning, I undertook a project to analyze it from three perspectives: text, picture, and music sentiment analysis, using Python programming language. In the text analysis, I examined the screenplay and dialogue for patterns and themes, while the picture analysis focused on cinematography and visual motifs. Lastly, I explored the music in the film, analyzing its emotional impact and how it enhances the viewing experience. Through this comprehensive analysis, we aim to gain a deeper understanding of the complex themes and emotions presented in the film.

P.S note that the text provided here are generated by chatgpt under my confirmation of details and context and analysis. So, basically it helped me to speed up the process of writing. 


## Text Data

### 1- Collecting Data

The code is intended to extract the subtitle text from the SRT file, perform sentiment analysis on the text using TextBlob, and store the sentiment scores for each sentence in a list. Thank you for correcting me.

The reason for this could be that there were no available screenplay files online for the movie being analyzed. Screenplay files are not always publicly available, especially for recent films. On the other hand, SRT subtitle files are commonly available and can be easily extracted from video files or downloaded from online sources.

Since the goal of the code is to perform sentiment analysis on the text of the movie, the SRT subtitle file serves as a suitable alternative to the screenplay file. While the SRT file does not contain the same level of detail as a screenplay file, it still provides a good source of textual content for analysis.



```python
import pandas as pd
import numpy as np
import pysrt
from textblob import TextBlob

srt = pysrt.open('vid.srt')

print(srt[5].text)
# Join all the subtitle text into a single string
text = ' '.join(sub.text for sub in srt)  

blob = TextBlob(text)

z=[]
def sensentiment(x):
    for i in x.sentences:
        z.append(i.sentiment)
        
sensentiment(blob)
```
### 2- Some quick look into data

Next step is to check the data with histograms of polarity and subjectivity.

```python
ax=df_sentiment["polarities"].hist(bins=20, density=True,stacked=False,color='teal', alpha=0.6)
df_sentiment["polarities"].plot(kind='density',color='black')
ax.set(xlabel='polarities')
plt.xlim(-1,1)
plt.show()

ax=df_sentiment["subjectivity"].hist(bins=20, density=True,stacked=False,color='teal', alpha=0.6)
df_sentiment["subjectivity"].plot(kind='density',color='black')
ax.set(xlabel='subjectivity')
plt.xlim(0,1)
plt.show()
```

These lines of code are creating histograms and density plots for the sentiment analysis results of a movie. Specifically, the code is analyzing two sentiment features: polarity and subjectivity.

Polarity and subjectivity are two common measures used in sentiment analysis to quantify the sentiment expressed in a text.

Polarity is a measure of the sentiment expressed in a text, with a range from -1 to 1, where -1 represents a completely negative sentiment, 0 represents a neutral sentiment, and 1 represents a completely positive sentiment. Polarity is calculated by analyzing the words and phrases used in the text to determine if they express a positive or negative sentiment.

Subjectivity, on the other hand, is a measure of the degree to which the text expresses an opinion or personal belief, rather than a fact. It ranges from 0 to 1, where 0 represents an objective statement or fact, and 1 represents a highly subjective statement expressing personal opinion or belief. Subjectivity is calculated by analyzing the use of words and phrases that indicate the presence of a personal opinion or belief, such as "I think", "I feel", or "in my opinion".

![1](https://user-images.githubusercontent.com/109058050/233825584-a37c47c5-f837-4529-9282-1328f7df0fd1.png)

![2](https://user-images.githubusercontent.com/109058050/233825585-cba7f66f-61a2-441d-aa63-dfdc957a84d6.png)

The fact that the average subjectivity score is near 0 suggests that the majority of the sentences in the movie are expressing objective statements or facts, rather than personal opinions or beliefs. This could indicate that the movie is presenting a narrative that is focused on presenting information or events in a straightforward and objective manner, rather than trying to persuade or influence the audience's emotions or opinions.

The average polarity score being between 0 and 0.25 suggests that the overall sentiment expressed in the movie is slightly positive, but not overwhelmingly so. This could indicate that the movie presents a balanced view of the events or themes being explored, rather than being overly biased or extreme in its portrayal. It is worth noting that the average polarity score is only one measure of sentiment, and it is important to look at the distribution of polarity scores across the movie to get a more complete picture of the sentiment expressed.

then:
```python
import nltk
tags = nltk.pos_tag(words)
df_tags=pd.DataFrame(tags,columns=['words','pos'])
df_tags=df_tags.groupby(['pos']).count().sort_values('words', ascending=False)
df_tags=df_tags.head()

import matplotlib.pyplot as plt
ax = df_t.plot(kind='bar')

# set the title and axis labels
ax.set_title('Word Frequencies')
ax.set_xlabel('Part of Speech')
ax.set_ylabel('Frequency')
plt.figure(figsize=(8, 20))
# display the plot
plt.show()
```
this code performs POS tagging on a list of words, creates a bar chart of the top five most frequent POS tags, and displys it.

![3](https://user-images.githubusercontent.com/109058050/233826770-bdbe45ba-6c7e-4f6b-b2a2-0aabe9ca0b35.png)

PRP stands for personal pronoun, NNP stands for proper noun, NN stands for noun, VBP stands for verb in present tense, and IN stands for preposition/subordinating conjunction.

A high frequency of PRP suggests that the characters in the movie speak using personal pronouns frequently, which may imply that they are expressing their feelings or thoughts more often than they are describing events or objects. Similarly, a high frequency of NNP may suggest that the movie has a significant focus on specific people, places, or things, such as proper names or specific locations. A high frequency of NN suggests that the characters in the movie are talking about objects or things more often than other parts of speech, and a high frequency of VBP suggests that the characters in the movie use present tense verbs frequently.

### 3- Word and Sentence Sentiment

The code uses the Natural Language Toolkit (nltk) library to perform sentiment analysis on words and sentences using the Vader Sentiment Intensity Analyzer.

The first block of code imports the necessary libraries, initializes the Sentiment Intensity Analyzer, and creates empty lists to store the word and sentence sentiments.

The second block of code uses a for loop to iterate over each word in a list of words (w). For each word, the code applies the polarity_scores method of the Sentiment Intensity Analyzer to obtain a dictionary of polarity scores (positive, negative, neutral, and compound). This dictionary is then appended to the w_sentiment list.

The third block of code uses another for loop to iterate over each sentence in a list of sentences (s). For each sentence, the code applies the polarity_scores method of the Sentiment Intensity Analyzer to obtain a dictionary of polarity scores. This dictionary is then appended to the s_sentiment list.

```python
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
analyzer = SentimentIntensityAnalyzer()
w_sentiment=[]
s_sentiment=[]
for word in w:
    w_sentiment.append(analyzer.polarity_scores(word))
    
for sentence in s:
    s_sentiment.append(analyzer.polarity_scores(str(sentence)))
```
The output of this code is two lists (w_sentiment and s_sentiment) containing dictionaries of polarity scores for each word and sentence, respectively. The polarity scores represent the degree of positive, negative, and neutral sentiment, as well as an overall compound score that combines all three.

```lua
w_sentiment:
[{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},
{'neg': 0.0, 'neu': 0.652, 'pos': 0.348, 'compound': 0.4215},
{'neg': 0.0, 'neu': 0.714, 'pos': 0.286, 'compound': 0.2732},
{'neg': 0.0, 'neu': 0.769, 'pos': 0.231, 'compound': 0.2732},
{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},
...

s_sentiment:
[{'neg': 0.0, 'neu': 0.734, 'pos': 0.266, 'compound': 0.4404},
{'neg': 0.0, 'neu': 0.761, 'pos': 0.239, 'compound': 0.4404},
{'neg': 0.13, 'neu': 0.599, 'pos': 0.271, 'compound': 0.4588},
{'neg': 0.0, 'neu': 0.841, 'pos': 0.159, 'compound': 0.34},
{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},
...]
```

These are the output of the VADER sentiment analysis tool, which is a lexicon and rule-based sentiment analysis tool specifically designed for social media.
The values in the dictionary correspond to:
   - neg: the negative sentiment score (from 0 to 1) of the input text
   - neu: the neutral sentiment score (from 0 to 1) of the input text
   - pos: the positive sentiment score (from 0 to 1) of the input text
   - compound: the overall sentiment score (from -1 to 1) of the input text, calculated by normalizing the scores of neg, neu, and pos and then summing them up.
   
```python
sentence_sentiment=[]
for i in range(len(s_sentiment)):
    sentence_sentiment.append(s_sentiment[i]['compound'])
sns.scatterplot(sentence_sentiment)
```
The code above creates a scatter plot using the Seaborn library in Python. It starts by iterating over a list called s_sentiment and appending the 'compound' value of each dictionary in the list to a new list called sentence_sentiment. The 'compound' value is a sentiment score that ranges between -1 (negative) and +1 (positive), indicating the overall sentiment of a text.

Once sentence_sentiment has been populated, it is passed as the data parameter to the sns.scatterplot() function, which creates a scatter plot with the sentiment score values on the y-axis. The x-axis values are simply the indices of the elements in sentence_sentiment.

![4](https://user-images.githubusercontent.com/109058050/233850896-0684614c-aeb4-4c2a-bdb8-fe49b18fb34c.png)


We may see that the sentiment of the sentences equally distributed. However, lets plot the regression line!!

```python
from sklearn.linear_model import LinearRegression
df = pd.DataFrame(s_sentiment)

X = df.index.values.reshape(-1, 1) # use index as predictor variable
y = df['compound'].values 

# create and fit the linear regression model
model = LinearRegression()
model.fit(X, y)

# predict the 'compound' values based on the predictor variable
predicted_compound = model.predict(X)

# plot the actual and predicted 'compound' values
import matplotlib.pyplot as plt
plt.scatter(X, y)
plt.plot(X, predicted_compound, color='red')
plt.xlabel('Time')
plt.ylabel('Compound')
plt.show()
```

![5](https://user-images.githubusercontent.com/109058050/233851011-a84e1a83-9da0-4772-be9d-583c546ce6cc.png)

 Linear regression red line indicates that during the movie, average sentiments of the sentences become more and more negative. 

Also we can be more specific with KMeans and KNeighbors. Here are the steps that the code is performing:

   - Import the necessary libraries: pandas, numpy, matplotlib, and scikit-learn.
   - Load the dataset into a pandas DataFrame object.
   - Extract the predictor variable (time) and the target variable (compound) from the DataFrame.
   - Apply k-means clustering to the target variable to group the data into 3 clusters.
   - Apply KNN regression to predict the 'compound' values for each data point based on the predictor variable.
   - Plot the actual 'compound' values with color-coded cluster labels.
   - Plot the predicted 'compound' values as red dots.
   
```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.neighbors import KNeighborsRegressor

# define the predictor and target variables
X = df.index.values.reshape(-1, 1) # use index as predictor variable
y = df['compound'].values # use 'compound' column as target variable

# apply k-means clustering to group the data into 3 clusters
kmeans = KMeans(n_clusters=3)
kmeans.fit(y.reshape(-1,1))
cluster_labels = kmeans.predict(y.reshape(-1,1))

# apply KNN regression to predict the 'compound' values for each data point
knn = KNeighborsRegressor(n_neighbors=6)
knn.fit(X, y)
predicted_compound = knn.predict(X)

# plot the actual 'compound' values with color-coded cluster labels
fig, ax = plt.subplots()
scatter = ax.scatter(X, y, c=cluster_labels)
legend = ax.legend(*scatter.legend_elements(),
                    loc="upper right", title="Clusters")
ax.add_artist(legend)
ax.set_xlabel('Time')
ax.set_ylabel('Compound')

# plot the predicted 'compound' values as red dots
plt.plot(X, predicted_compound, color='red')
plt.show()
```
and lets see the results:

![7](https://user-images.githubusercontent.com/109058050/233851168-ec142dbc-9882-4637-b58c-75e05dacdaae.png)

KNN is a non-parametric machine learning algorithm that is often used for regression tasks.
The KNN algorithm works by finding the k closest data points to the query point (in this case, a specific 'time' value), and then taking the average (or weighted average) of the target variable values for those k neighbors. The predicted value for the query point is then set to this average.

To interpret the predicted 'compound' values, you can compare them to the actual 'compound' values plotted in the same figure. If the predicted values closely follow the pattern of the actual values, it suggests that the KNN regression model has successfully captured the underlying relationship between the predictor variable ('time') and the target variable ('compound') in the dataset.

This means that averaged predicted compound sentiments of sentences in the movie fluctuated and touched the extream border points of the nuetral sentiment section. **** I will explain it later. 
